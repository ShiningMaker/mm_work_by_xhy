2023-02-05 22:43:59,719 - mmcls - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.16 (default, Jan 17 2023, 23:13:24) [GCC 11.2.0]
CUDA available: True
GPU 0: NVIDIA GeForce RTX 3090
CUDA_HOME: /data/apps/cuda/11.1
NVCC: Cuda compilation tools, release 11.1, V11.1.74
GCC: gcc (GCC) 7.3.0
PyTorch: 1.10.0+cu111
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.0+cu111
OpenCV: 4.7.0
MMCV: 1.7.0
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 11.1
MMClassification: 0.25.0+
------------------------------------------------------------

2023-02-05 22:43:59,720 - mmcls - INFO - Distributed training: False
2023-02-05 22:44:00,128 - mmcls - INFO - Config:
model = dict(
    type='ImageClassifier',
    backbone=dict(
        type='SwinTransformer', arch='tiny', img_size=224, drop_path_rate=0.2),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='LinearClsHead',
        num_classes=5,
        in_channels=768,
        init_cfg=None,
        loss=dict(
            type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
        cal_acc=False),
    init_cfg=[
        dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.0),
        dict(type='Constant', layer='LayerNorm', val=1.0, bias=0.0)
    ],
    train_cfg=dict(augments=[
        dict(type='BatchMixup', alpha=0.8, num_classes=5, prob=0.5),
        dict(type='BatchCutMix', alpha=1.0, num_classes=5, prob=0.5)
    ]))
rand_increasing_policies = [
    dict(type='AutoContrast'),
    dict(type='Equalize'),
    dict(type='Invert'),
    dict(type='Rotate', magnitude_key='angle', magnitude_range=(0, 30)),
    dict(type='Posterize', magnitude_key='bits', magnitude_range=(4, 0)),
    dict(type='Solarize', magnitude_key='thr', magnitude_range=(256, 0)),
    dict(
        type='SolarizeAdd',
        magnitude_key='magnitude',
        magnitude_range=(0, 110)),
    dict(
        type='ColorTransform',
        magnitude_key='magnitude',
        magnitude_range=(0, 0.9)),
    dict(type='Contrast', magnitude_key='magnitude', magnitude_range=(0, 0.9)),
    dict(
        type='Brightness', magnitude_key='magnitude',
        magnitude_range=(0, 0.9)),
    dict(
        type='Sharpness', magnitude_key='magnitude', magnitude_range=(0, 0.9)),
    dict(
        type='Shear',
        magnitude_key='magnitude',
        magnitude_range=(0, 0.3),
        direction='horizontal'),
    dict(
        type='Shear',
        magnitude_key='magnitude',
        magnitude_range=(0, 0.3),
        direction='vertical'),
    dict(
        type='Translate',
        magnitude_key='magnitude',
        magnitude_range=(0, 0.45),
        direction='horizontal'),
    dict(
        type='Translate',
        magnitude_key='magnitude',
        magnitude_range=(0, 0.45),
        direction='vertical')
]
dataset_type = 'ImageNet'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='RandomResizedCrop',
        size=224,
        backend='pillow',
        interpolation='bicubic'),
    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
    dict(
        type='RandAugment',
        policies=[
            dict(type='AutoContrast'),
            dict(type='Equalize'),
            dict(type='Invert'),
            dict(
                type='Rotate', magnitude_key='angle', magnitude_range=(0, 30)),
            dict(
                type='Posterize', magnitude_key='bits',
                magnitude_range=(4, 0)),
            dict(
                type='Solarize', magnitude_key='thr',
                magnitude_range=(256, 0)),
            dict(
                type='SolarizeAdd',
                magnitude_key='magnitude',
                magnitude_range=(0, 110)),
            dict(
                type='ColorTransform',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.9)),
            dict(
                type='Contrast',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.9)),
            dict(
                type='Brightness',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.9)),
            dict(
                type='Sharpness',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.9)),
            dict(
                type='Shear',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.3),
                direction='horizontal'),
            dict(
                type='Shear',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.3),
                direction='vertical'),
            dict(
                type='Translate',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.45),
                direction='horizontal'),
            dict(
                type='Translate',
                magnitude_key='magnitude',
                magnitude_range=(0, 0.45),
                direction='vertical')
        ],
        num_policies=2,
        total_level=10,
        magnitude_level=9,
        magnitude_std=0.5,
        hparams=dict(pad_val=[104, 116, 124], interpolation='bicubic')),
    dict(
        type='RandomErasing',
        erase_prob=0.25,
        mode='rand',
        min_area_ratio=0.02,
        max_area_ratio=0.3333333333333333,
        fill_color=[103.53, 116.28, 123.675],
        fill_std=[57.375, 57.12, 58.395]),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='ToTensor', keys=['gt_label']),
    dict(type='Collect', keys=['img', 'gt_label'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='Resize',
        size=(256, -1),
        backend='pillow',
        interpolation='bicubic'),
    dict(type='CenterCrop', crop_size=224),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='Collect', keys=['img'])
]
data = dict(
    samples_per_gpu=32,
    workers_per_gpu=2,
    train=dict(
        type='ImageNet',
        data_prefix='/HOME/scz0aru/run/mmclassification/data/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='RandomResizedCrop',
                size=224,
                backend='pillow',
                interpolation='bicubic'),
            dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
            dict(
                type='RandAugment',
                policies=[
                    dict(type='AutoContrast'),
                    dict(type='Equalize'),
                    dict(type='Invert'),
                    dict(
                        type='Rotate',
                        magnitude_key='angle',
                        magnitude_range=(0, 30)),
                    dict(
                        type='Posterize',
                        magnitude_key='bits',
                        magnitude_range=(4, 0)),
                    dict(
                        type='Solarize',
                        magnitude_key='thr',
                        magnitude_range=(256, 0)),
                    dict(
                        type='SolarizeAdd',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 110)),
                    dict(
                        type='ColorTransform',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.9)),
                    dict(
                        type='Contrast',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.9)),
                    dict(
                        type='Brightness',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.9)),
                    dict(
                        type='Sharpness',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.9)),
                    dict(
                        type='Shear',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.3),
                        direction='horizontal'),
                    dict(
                        type='Shear',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.3),
                        direction='vertical'),
                    dict(
                        type='Translate',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.45),
                        direction='horizontal'),
                    dict(
                        type='Translate',
                        magnitude_key='magnitude',
                        magnitude_range=(0, 0.45),
                        direction='vertical')
                ],
                num_policies=2,
                total_level=10,
                magnitude_level=9,
                magnitude_std=0.5,
                hparams=dict(pad_val=[104, 116, 124],
                             interpolation='bicubic')),
            dict(
                type='RandomErasing',
                erase_prob=0.25,
                mode='rand',
                min_area_ratio=0.02,
                max_area_ratio=0.3333333333333333,
                fill_color=[103.53, 116.28, 123.675],
                fill_std=[57.375, 57.12, 58.395]),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='ToTensor', keys=['gt_label']),
            dict(type='Collect', keys=['img', 'gt_label'])
        ],
        ann_file=None,
        classes='/HOME/scz0aru/run/mmclassification/data/classes.txt'),
    val=dict(
        type='ImageNet',
        data_prefix='/HOME/scz0aru/run/mmclassification/data/val',
        ann_file=None,
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='Resize',
                size=(256, -1),
                backend='pillow',
                interpolation='bicubic'),
            dict(type='CenterCrop', crop_size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ],
        classes='/HOME/scz0aru/run/mmclassification/data/classes.txt'),
    test=dict(
        type='ImageNet',
        data_prefix='data/imagenet/val',
        ann_file='data/imagenet/meta/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='Resize',
                size=(256, -1),
                backend='pillow',
                interpolation='bicubic'),
            dict(type='CenterCrop', crop_size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ]))
evaluation = dict(interval=1, metric='accuracy')
checkpoint_config = dict(interval=1)
log_config = dict(interval=1, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/HOME/scz0aru/run/mmclassification/checkpoints/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth'
resume_from = None
workflow = [('train', 1)]
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(policy='step', step=[1])
runner = dict(type='EpochBasedRunner', max_epochs=5)
work_dir = 'work/swin-tiny-flower'
gpu_ids = [0]

2023-02-05 22:44:00,132 - mmcls - INFO - Set random seed to 1002682146, deterministic: False
2023-02-05 22:44:00,312 - mmcls - INFO - initialize ImageClassifier with init_cfg [{'type': 'TruncNormal', 'layer': 'Linear', 'std': 0.02, 'bias': 0.0}, {'type': 'Constant', 'layer': 'LayerNorm', 'val': 1.0, 'bias': 0.0}]
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([96, 3, 4, 4]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.patch_embed.projection.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.patch_embed.norm.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.patch_embed.norm.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([288]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([96, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([384, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([96, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.norm1.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([288]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([96, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.norm2.weight - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([384, 96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([96, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([96]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.0.downsample.norm.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.downsample.norm.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.0.downsample.reduction.weight - torch.Size([192, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.norm2.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.1.downsample.norm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.downsample.norm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.1.downsample.reduction.weight - torch.Size([384, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.norm1.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1152]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([384, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([384, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([384]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.2.downsample.norm.weight - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.downsample.norm.bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.2.downsample.reduction.weight - torch.Size([768, 1536]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.norm1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.norm1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in WindowMSA  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.norm2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

backbone.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

backbone.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of ImageClassifier  

head.fc.weight - torch.Size([5, 768]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 

head.fc.bias - torch.Size([5]): 
TruncNormalInit: a=-2, b=2, mean=0, std=0.02, bias=0.0 
2023-02-05 22:44:05,874 - mmcls - INFO - load checkpoint from local path: /HOME/scz0aru/run/mmclassification/checkpoints/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
2023-02-05 22:44:06,110 - mmcls - WARNING - The model and loaded state dict do not match exactly

size mismatch for head.fc.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([5, 768]).
size mismatch for head.fc.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([5]).
2023-02-05 22:44:06,111 - mmcls - INFO - Start running, host: scz0aru@g0015, work_dir: /data/run01/scz0aru/work/swin-tiny-flower
2023-02-05 22:44:06,112 - mmcls - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-05 22:44:06,112 - mmcls - INFO - workflow: [('train', 1)], max: 5 epochs
2023-02-05 22:44:06,112 - mmcls - INFO - Checkpoints will be saved to /data/run01/scz0aru/work/swin-tiny-flower by HardDiskBackend.
2023-02-05 22:44:08,918 - mmcls - INFO - Epoch [1][1/72]	lr: 1.000e-03, eta: 0:16:45, time: 2.801, data_time: 2.549, memory: 3669, loss: 1.6282
2023-02-05 22:44:09,046 - mmcls - INFO - Epoch [1][2/72]	lr: 1.000e-03, eta: 0:08:43, time: 0.122, data_time: 0.003, memory: 3883, loss: 1.6387
2023-02-05 22:44:09,174 - mmcls - INFO - Epoch [1][3/72]	lr: 1.000e-03, eta: 0:06:02, time: 0.127, data_time: 0.008, memory: 3883, loss: 1.7204
2023-02-05 22:44:09,300 - mmcls - INFO - Epoch [1][4/72]	lr: 1.000e-03, eta: 0:04:42, time: 0.129, data_time: 0.009, memory: 3883, loss: 1.6233
2023-02-05 22:44:09,425 - mmcls - INFO - Epoch [1][5/72]	lr: 1.000e-03, eta: 0:03:54, time: 0.123, data_time: 0.006, memory: 3883, loss: 1.5906
2023-02-05 22:44:09,551 - mmcls - INFO - Epoch [1][6/72]	lr: 1.000e-03, eta: 0:03:22, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.6271
2023-02-05 22:44:09,677 - mmcls - INFO - Epoch [1][7/72]	lr: 1.000e-03, eta: 0:02:59, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.5720
2023-02-05 22:44:09,802 - mmcls - INFO - Epoch [1][8/72]	lr: 1.000e-03, eta: 0:02:41, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.5961
2023-02-05 22:44:09,928 - mmcls - INFO - Epoch [1][9/72]	lr: 1.000e-03, eta: 0:02:28, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.6938
2023-02-05 22:44:10,054 - mmcls - INFO - Epoch [1][10/72]	lr: 1.000e-03, eta: 0:02:17, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.5560
2023-02-05 22:44:10,180 - mmcls - INFO - Epoch [1][11/72]	lr: 1.000e-03, eta: 0:02:08, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.5890
2023-02-05 22:44:10,307 - mmcls - INFO - Epoch [1][12/72]	lr: 1.000e-03, eta: 0:02:01, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.5564
2023-02-05 22:44:10,433 - mmcls - INFO - Epoch [1][13/72]	lr: 1.000e-03, eta: 0:01:55, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.6171
2023-02-05 22:44:10,559 - mmcls - INFO - Epoch [1][14/72]	lr: 1.000e-03, eta: 0:01:49, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.4792
2023-02-05 22:44:10,687 - mmcls - INFO - Epoch [1][15/72]	lr: 1.000e-03, eta: 0:01:44, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.5874
2023-02-05 22:44:10,813 - mmcls - INFO - Epoch [1][16/72]	lr: 1.000e-03, eta: 0:01:40, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.5486
2023-02-05 22:44:10,939 - mmcls - INFO - Epoch [1][17/72]	lr: 1.000e-03, eta: 0:01:37, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.4795
2023-02-05 22:44:11,066 - mmcls - INFO - Epoch [1][18/72]	lr: 1.000e-03, eta: 0:01:33, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.5184
2023-02-05 22:44:11,192 - mmcls - INFO - Epoch [1][19/72]	lr: 1.000e-03, eta: 0:01:30, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.4483
2023-02-05 22:44:11,319 - mmcls - INFO - Epoch [1][20/72]	lr: 1.000e-03, eta: 0:01:28, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.4188
2023-02-05 22:44:11,447 - mmcls - INFO - Epoch [1][21/72]	lr: 1.000e-03, eta: 0:01:25, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.5998
2023-02-05 22:44:11,573 - mmcls - INFO - Epoch [1][22/72]	lr: 1.000e-03, eta: 0:01:23, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2968
2023-02-05 22:44:11,701 - mmcls - INFO - Epoch [1][23/72]	lr: 1.000e-03, eta: 0:01:21, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.4746
2023-02-05 22:44:11,827 - mmcls - INFO - Epoch [1][24/72]	lr: 1.000e-03, eta: 0:01:19, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.3416
2023-02-05 22:44:11,954 - mmcls - INFO - Epoch [1][25/72]	lr: 1.000e-03, eta: 0:01:18, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3756
2023-02-05 22:44:12,080 - mmcls - INFO - Epoch [1][26/72]	lr: 1.000e-03, eta: 0:01:16, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2505
2023-02-05 22:44:12,207 - mmcls - INFO - Epoch [1][27/72]	lr: 1.000e-03, eta: 0:01:15, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.3168
2023-02-05 22:44:12,334 - mmcls - INFO - Epoch [1][28/72]	lr: 1.000e-03, eta: 0:01:13, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.3351
2023-02-05 22:44:12,460 - mmcls - INFO - Epoch [1][29/72]	lr: 1.000e-03, eta: 0:01:12, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.3221
2023-02-05 22:44:12,587 - mmcls - INFO - Epoch [1][30/72]	lr: 1.000e-03, eta: 0:01:11, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3262
2023-02-05 22:44:12,716 - mmcls - INFO - Epoch [1][31/72]	lr: 1.000e-03, eta: 0:01:09, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.3912
2023-02-05 22:44:12,842 - mmcls - INFO - Epoch [1][32/72]	lr: 1.000e-03, eta: 0:01:08, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1915
2023-02-05 22:44:12,969 - mmcls - INFO - Epoch [1][33/72]	lr: 1.000e-03, eta: 0:01:07, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.4018
2023-02-05 22:44:13,096 - mmcls - INFO - Epoch [1][34/72]	lr: 1.000e-03, eta: 0:01:06, time: 0.125, data_time: 0.006, memory: 3885, loss: 1.2823
2023-02-05 22:44:13,223 - mmcls - INFO - Epoch [1][35/72]	lr: 1.000e-03, eta: 0:01:05, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2113
2023-02-05 22:44:13,351 - mmcls - INFO - Epoch [1][36/72]	lr: 1.000e-03, eta: 0:01:05, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.1161
2023-02-05 22:44:13,478 - mmcls - INFO - Epoch [1][37/72]	lr: 1.000e-03, eta: 0:01:04, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1302
2023-02-05 22:44:13,604 - mmcls - INFO - Epoch [1][38/72]	lr: 1.000e-03, eta: 0:01:03, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3013
2023-02-05 22:44:13,731 - mmcls - INFO - Epoch [1][39/72]	lr: 1.000e-03, eta: 0:01:02, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3894
2023-02-05 22:44:13,857 - mmcls - INFO - Epoch [1][40/72]	lr: 1.000e-03, eta: 0:01:01, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1356
2023-02-05 22:44:13,984 - mmcls - INFO - Epoch [1][41/72]	lr: 1.000e-03, eta: 0:01:01, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.4341
2023-02-05 22:44:14,111 - mmcls - INFO - Epoch [1][42/72]	lr: 1.000e-03, eta: 0:01:00, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.3866
2023-02-05 22:44:14,237 - mmcls - INFO - Epoch [1][43/72]	lr: 1.000e-03, eta: 0:00:59, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0904
2023-02-05 22:44:14,367 - mmcls - INFO - Epoch [1][44/72]	lr: 1.000e-03, eta: 0:00:59, time: 0.129, data_time: 0.009, memory: 3885, loss: 1.1033
2023-02-05 22:44:14,494 - mmcls - INFO - Epoch [1][45/72]	lr: 1.000e-03, eta: 0:00:58, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9791
2023-02-05 22:44:14,621 - mmcls - INFO - Epoch [1][46/72]	lr: 1.000e-03, eta: 0:00:57, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2670
2023-02-05 22:44:14,749 - mmcls - INFO - Epoch [1][47/72]	lr: 1.000e-03, eta: 0:00:57, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1909
2023-02-05 22:44:14,875 - mmcls - INFO - Epoch [1][48/72]	lr: 1.000e-03, eta: 0:00:56, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1915
2023-02-05 22:44:15,002 - mmcls - INFO - Epoch [1][49/72]	lr: 1.000e-03, eta: 0:00:56, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2585
2023-02-05 22:44:15,128 - mmcls - INFO - Epoch [1][50/72]	lr: 1.000e-03, eta: 0:00:55, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2155
2023-02-05 22:44:15,257 - mmcls - INFO - Epoch [1][51/72]	lr: 1.000e-03, eta: 0:00:55, time: 0.129, data_time: 0.009, memory: 3885, loss: 1.2856
2023-02-05 22:44:15,384 - mmcls - INFO - Epoch [1][52/72]	lr: 1.000e-03, eta: 0:00:54, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1930
2023-02-05 22:44:15,511 - mmcls - INFO - Epoch [1][53/72]	lr: 1.000e-03, eta: 0:00:54, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2875
2023-02-05 22:44:15,638 - mmcls - INFO - Epoch [1][54/72]	lr: 1.000e-03, eta: 0:00:53, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1550
2023-02-05 22:44:15,765 - mmcls - INFO - Epoch [1][55/72]	lr: 1.000e-03, eta: 0:00:53, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0582
2023-02-05 22:44:15,891 - mmcls - INFO - Epoch [1][56/72]	lr: 1.000e-03, eta: 0:00:53, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2748
2023-02-05 22:44:16,018 - mmcls - INFO - Epoch [1][57/72]	lr: 1.000e-03, eta: 0:00:52, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1266
2023-02-05 22:44:16,144 - mmcls - INFO - Epoch [1][58/72]	lr: 1.000e-03, eta: 0:00:52, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0620
2023-02-05 22:44:16,272 - mmcls - INFO - Epoch [1][59/72]	lr: 1.000e-03, eta: 0:00:51, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.2454
2023-02-05 22:44:16,399 - mmcls - INFO - Epoch [1][60/72]	lr: 1.000e-03, eta: 0:00:51, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2176
2023-02-05 22:44:16,526 - mmcls - INFO - Epoch [1][61/72]	lr: 1.000e-03, eta: 0:00:50, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9846
2023-02-05 22:44:16,654 - mmcls - INFO - Epoch [1][62/72]	lr: 1.000e-03, eta: 0:00:50, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1021
2023-02-05 22:44:16,780 - mmcls - INFO - Epoch [1][63/72]	lr: 1.000e-03, eta: 0:00:50, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1831
2023-02-05 22:44:16,907 - mmcls - INFO - Epoch [1][64/72]	lr: 1.000e-03, eta: 0:00:49, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8892
2023-02-05 22:44:17,034 - mmcls - INFO - Epoch [1][65/72]	lr: 1.000e-03, eta: 0:00:49, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8987
2023-02-05 22:44:17,161 - mmcls - INFO - Epoch [1][66/72]	lr: 1.000e-03, eta: 0:00:49, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3271
2023-02-05 22:44:17,288 - mmcls - INFO - Epoch [1][67/72]	lr: 1.000e-03, eta: 0:00:48, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.2311
2023-02-05 22:44:17,418 - mmcls - INFO - Epoch [1][68/72]	lr: 1.000e-03, eta: 0:00:48, time: 0.130, data_time: 0.008, memory: 3885, loss: 1.0852
2023-02-05 22:44:17,545 - mmcls - INFO - Epoch [1][69/72]	lr: 1.000e-03, eta: 0:00:48, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2037
2023-02-05 22:44:17,670 - mmcls - INFO - Epoch [1][70/72]	lr: 1.000e-03, eta: 0:00:47, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9642
2023-02-05 22:44:17,796 - mmcls - INFO - Epoch [1][71/72]	lr: 1.000e-03, eta: 0:00:47, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0295
2023-02-05 22:44:17,839 - mmcls - INFO - Epoch [1][72/72]	lr: 1.000e-03, eta: 0:00:46, time: 0.050, data_time: 0.008, memory: 3885, loss: 1.0360
2023-02-05 22:44:17,840 - mmcls - INFO - Saving checkpoint at 1 epochs
2023-02-05 22:44:22,865 - mmcls - INFO - Epoch(val) [1][18]	accuracy_top-1: 92.1329, accuracy_top-5: 100.0000
2023-02-05 22:44:25,204 - mmcls - INFO - Epoch [2][1/72]	lr: 1.000e-04, eta: 0:00:55, time: 2.329, data_time: 2.201, memory: 3885, loss: 1.0364
2023-02-05 22:44:25,341 - mmcls - INFO - Epoch [2][2/72]	lr: 1.000e-04, eta: 0:00:54, time: 0.137, data_time: 0.009, memory: 3885, loss: 1.0731
2023-02-05 22:44:25,470 - mmcls - INFO - Epoch [2][3/72]	lr: 1.000e-04, eta: 0:00:54, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1476
2023-02-05 22:44:25,597 - mmcls - INFO - Epoch [2][4/72]	lr: 1.000e-04, eta: 0:00:53, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2310
2023-02-05 22:44:25,725 - mmcls - INFO - Epoch [2][5/72]	lr: 1.000e-04, eta: 0:00:53, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2855
2023-02-05 22:44:25,852 - mmcls - INFO - Epoch [2][6/72]	lr: 1.000e-04, eta: 0:00:53, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.9502
2023-02-05 22:44:25,979 - mmcls - INFO - Epoch [2][7/72]	lr: 1.000e-04, eta: 0:00:52, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0290
2023-02-05 22:44:26,107 - mmcls - INFO - Epoch [2][8/72]	lr: 1.000e-04, eta: 0:00:52, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.8679
2023-02-05 22:44:26,234 - mmcls - INFO - Epoch [2][9/72]	lr: 1.000e-04, eta: 0:00:51, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1834
2023-02-05 22:44:26,363 - mmcls - INFO - Epoch [2][10/72]	lr: 1.000e-04, eta: 0:00:51, time: 0.129, data_time: 0.009, memory: 3885, loss: 1.2181
2023-02-05 22:44:26,492 - mmcls - INFO - Epoch [2][11/72]	lr: 1.000e-04, eta: 0:00:51, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0306
2023-02-05 22:44:26,618 - mmcls - INFO - Epoch [2][12/72]	lr: 1.000e-04, eta: 0:00:50, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0637
2023-02-05 22:44:26,745 - mmcls - INFO - Epoch [2][13/72]	lr: 1.000e-04, eta: 0:00:50, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3393
2023-02-05 22:44:26,872 - mmcls - INFO - Epoch [2][14/72]	lr: 1.000e-04, eta: 0:00:50, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2130
2023-02-05 22:44:27,000 - mmcls - INFO - Epoch [2][15/72]	lr: 1.000e-04, eta: 0:00:49, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0618
2023-02-05 22:44:27,126 - mmcls - INFO - Epoch [2][16/72]	lr: 1.000e-04, eta: 0:00:49, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2510
2023-02-05 22:44:27,254 - mmcls - INFO - Epoch [2][17/72]	lr: 1.000e-04, eta: 0:00:49, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.8765
2023-02-05 22:44:27,381 - mmcls - INFO - Epoch [2][18/72]	lr: 1.000e-04, eta: 0:00:48, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.2630
2023-02-05 22:44:27,509 - mmcls - INFO - Epoch [2][19/72]	lr: 1.000e-04, eta: 0:00:48, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1797
2023-02-05 22:44:27,636 - mmcls - INFO - Epoch [2][20/72]	lr: 1.000e-04, eta: 0:00:48, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2560
2023-02-05 22:44:27,763 - mmcls - INFO - Epoch [2][21/72]	lr: 1.000e-04, eta: 0:00:47, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0773
2023-02-05 22:44:27,892 - mmcls - INFO - Epoch [2][22/72]	lr: 1.000e-04, eta: 0:00:47, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2289
2023-02-05 22:44:28,020 - mmcls - INFO - Epoch [2][23/72]	lr: 1.000e-04, eta: 0:00:47, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1698
2023-02-05 22:44:28,148 - mmcls - INFO - Epoch [2][24/72]	lr: 1.000e-04, eta: 0:00:46, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.8658
2023-02-05 22:44:28,277 - mmcls - INFO - Epoch [2][25/72]	lr: 1.000e-04, eta: 0:00:46, time: 0.129, data_time: 0.008, memory: 3885, loss: 0.9434
2023-02-05 22:44:28,405 - mmcls - INFO - Epoch [2][26/72]	lr: 1.000e-04, eta: 0:00:46, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.2185
2023-02-05 22:44:28,532 - mmcls - INFO - Epoch [2][27/72]	lr: 1.000e-04, eta: 0:00:45, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2698
2023-02-05 22:44:28,660 - mmcls - INFO - Epoch [2][28/72]	lr: 1.000e-04, eta: 0:00:45, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.9011
2023-02-05 22:44:28,787 - mmcls - INFO - Epoch [2][29/72]	lr: 1.000e-04, eta: 0:00:45, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2590
2023-02-05 22:44:28,915 - mmcls - INFO - Epoch [2][30/72]	lr: 1.000e-04, eta: 0:00:44, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0295
2023-02-05 22:44:29,044 - mmcls - INFO - Epoch [2][31/72]	lr: 1.000e-04, eta: 0:00:44, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1757
2023-02-05 22:44:29,171 - mmcls - INFO - Epoch [2][32/72]	lr: 1.000e-04, eta: 0:00:44, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9919
2023-02-05 22:44:29,299 - mmcls - INFO - Epoch [2][33/72]	lr: 1.000e-04, eta: 0:00:44, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.2266
2023-02-05 22:44:29,428 - mmcls - INFO - Epoch [2][34/72]	lr: 1.000e-04, eta: 0:00:43, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2119
2023-02-05 22:44:29,557 - mmcls - INFO - Epoch [2][35/72]	lr: 1.000e-04, eta: 0:00:43, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1586
2023-02-05 22:44:29,684 - mmcls - INFO - Epoch [2][36/72]	lr: 1.000e-04, eta: 0:00:43, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1694
2023-02-05 22:44:29,812 - mmcls - INFO - Epoch [2][37/72]	lr: 1.000e-04, eta: 0:00:42, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1343
2023-02-05 22:44:29,939 - mmcls - INFO - Epoch [2][38/72]	lr: 1.000e-04, eta: 0:00:42, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8526
2023-02-05 22:44:30,067 - mmcls - INFO - Epoch [2][39/72]	lr: 1.000e-04, eta: 0:00:42, time: 0.129, data_time: 0.008, memory: 3885, loss: 0.7997
2023-02-05 22:44:30,196 - mmcls - INFO - Epoch [2][40/72]	lr: 1.000e-04, eta: 0:00:42, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1362
2023-02-05 22:44:30,324 - mmcls - INFO - Epoch [2][41/72]	lr: 1.000e-04, eta: 0:00:41, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.9227
2023-02-05 22:44:30,452 - mmcls - INFO - Epoch [2][42/72]	lr: 1.000e-04, eta: 0:00:41, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1365
2023-02-05 22:44:30,580 - mmcls - INFO - Epoch [2][43/72]	lr: 1.000e-04, eta: 0:00:41, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1123
2023-02-05 22:44:30,707 - mmcls - INFO - Epoch [2][44/72]	lr: 1.000e-04, eta: 0:00:41, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9731
2023-02-05 22:44:30,835 - mmcls - INFO - Epoch [2][45/72]	lr: 1.000e-04, eta: 0:00:40, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2517
2023-02-05 22:44:30,962 - mmcls - INFO - Epoch [2][46/72]	lr: 1.000e-04, eta: 0:00:40, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9669
2023-02-05 22:44:31,089 - mmcls - INFO - Epoch [2][47/72]	lr: 1.000e-04, eta: 0:00:40, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.6619
2023-02-05 22:44:31,218 - mmcls - INFO - Epoch [2][48/72]	lr: 1.000e-04, eta: 0:00:40, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0061
2023-02-05 22:44:31,346 - mmcls - INFO - Epoch [2][49/72]	lr: 1.000e-04, eta: 0:00:39, time: 0.129, data_time: 0.009, memory: 3885, loss: 0.8982
2023-02-05 22:44:31,474 - mmcls - INFO - Epoch [2][50/72]	lr: 1.000e-04, eta: 0:00:39, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.9686
2023-02-05 22:44:31,603 - mmcls - INFO - Epoch [2][51/72]	lr: 1.000e-04, eta: 0:00:39, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1849
2023-02-05 22:44:31,730 - mmcls - INFO - Epoch [2][52/72]	lr: 1.000e-04, eta: 0:00:39, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0683
2023-02-05 22:44:31,859 - mmcls - INFO - Epoch [2][53/72]	lr: 1.000e-04, eta: 0:00:38, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1818
2023-02-05 22:44:31,987 - mmcls - INFO - Epoch [2][54/72]	lr: 1.000e-04, eta: 0:00:38, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2608
2023-02-05 22:44:32,116 - mmcls - INFO - Epoch [2][55/72]	lr: 1.000e-04, eta: 0:00:38, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.1251
2023-02-05 22:44:32,244 - mmcls - INFO - Epoch [2][56/72]	lr: 1.000e-04, eta: 0:00:38, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.7404
2023-02-05 22:44:32,374 - mmcls - INFO - Epoch [2][57/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.129, data_time: 0.010, memory: 3885, loss: 0.9607
2023-02-05 22:44:32,500 - mmcls - INFO - Epoch [2][58/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1791
2023-02-05 22:44:32,629 - mmcls - INFO - Epoch [2][59/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1384
2023-02-05 22:44:32,756 - mmcls - INFO - Epoch [2][60/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1553
2023-02-05 22:44:32,883 - mmcls - INFO - Epoch [2][61/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8445
2023-02-05 22:44:33,010 - mmcls - INFO - Epoch [2][62/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1593
2023-02-05 22:44:33,138 - mmcls - INFO - Epoch [2][63/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.3425
2023-02-05 22:44:33,265 - mmcls - INFO - Epoch [2][64/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8368
2023-02-05 22:44:33,394 - mmcls - INFO - Epoch [2][65/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.129, data_time: 0.010, memory: 3885, loss: 1.2628
2023-02-05 22:44:33,522 - mmcls - INFO - Epoch [2][66/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0857
2023-02-05 22:44:33,651 - mmcls - INFO - Epoch [2][67/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.129, data_time: 0.008, memory: 3885, loss: 0.8663
2023-02-05 22:44:33,785 - mmcls - INFO - Epoch [2][68/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.135, data_time: 0.008, memory: 3885, loss: 1.1821
2023-02-05 22:44:33,911 - mmcls - INFO - Epoch [2][69/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1341
2023-02-05 22:44:34,038 - mmcls - INFO - Epoch [2][70/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2563
2023-02-05 22:44:34,164 - mmcls - INFO - Epoch [2][71/72]	lr: 1.000e-04, eta: 0:00:34, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2585
2023-02-05 22:44:34,202 - mmcls - INFO - Epoch [2][72/72]	lr: 1.000e-04, eta: 0:00:34, time: 0.045, data_time: 0.008, memory: 3885, loss: 0.8368
2023-02-05 22:44:34,203 - mmcls - INFO - Saving checkpoint at 2 epochs
2023-02-05 22:44:35,947 - mmcls - INFO - Epoch(val) [2][18]	accuracy_top-1: 93.5315, accuracy_top-5: 100.0000
2023-02-05 22:44:38,280 - mmcls - INFO - Epoch [3][1/72]	lr: 1.000e-04, eta: 0:00:37, time: 2.324, data_time: 2.196, memory: 3885, loss: 0.8838
2023-02-05 22:44:38,412 - mmcls - INFO - Epoch [3][2/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.133, data_time: 0.010, memory: 3885, loss: 1.0566
2023-02-05 22:44:38,540 - mmcls - INFO - Epoch [3][3/72]	lr: 1.000e-04, eta: 0:00:37, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1549
2023-02-05 22:44:38,667 - mmcls - INFO - Epoch [3][4/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0229
2023-02-05 22:44:38,796 - mmcls - INFO - Epoch [3][5/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1212
2023-02-05 22:44:38,922 - mmcls - INFO - Epoch [3][6/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1207
2023-02-05 22:44:39,048 - mmcls - INFO - Epoch [3][7/72]	lr: 1.000e-04, eta: 0:00:36, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8796
2023-02-05 22:44:39,176 - mmcls - INFO - Epoch [3][8/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0617
2023-02-05 22:44:39,302 - mmcls - INFO - Epoch [3][9/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0367
2023-02-05 22:44:39,430 - mmcls - INFO - Epoch [3][10/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.128, data_time: 0.010, memory: 3885, loss: 1.1815
2023-02-05 22:44:39,557 - mmcls - INFO - Epoch [3][11/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.1445
2023-02-05 22:44:39,684 - mmcls - INFO - Epoch [3][12/72]	lr: 1.000e-04, eta: 0:00:35, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0005
2023-02-05 22:44:39,812 - mmcls - INFO - Epoch [3][13/72]	lr: 1.000e-04, eta: 0:00:34, time: 0.128, data_time: 0.010, memory: 3885, loss: 0.7632
2023-02-05 22:44:39,940 - mmcls - INFO - Epoch [3][14/72]	lr: 1.000e-04, eta: 0:00:34, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0908
2023-02-05 22:44:40,066 - mmcls - INFO - Epoch [3][15/72]	lr: 1.000e-04, eta: 0:00:34, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0928
2023-02-05 22:44:40,193 - mmcls - INFO - Epoch [3][16/72]	lr: 1.000e-04, eta: 0:00:34, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2108
2023-02-05 22:44:40,319 - mmcls - INFO - Epoch [3][17/72]	lr: 1.000e-04, eta: 0:00:33, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0509
2023-02-05 22:44:40,446 - mmcls - INFO - Epoch [3][18/72]	lr: 1.000e-04, eta: 0:00:33, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0042
2023-02-05 22:44:40,572 - mmcls - INFO - Epoch [3][19/72]	lr: 1.000e-04, eta: 0:00:33, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1927
2023-02-05 22:44:40,699 - mmcls - INFO - Epoch [3][20/72]	lr: 1.000e-04, eta: 0:00:33, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1122
2023-02-05 22:44:40,826 - mmcls - INFO - Epoch [3][21/72]	lr: 1.000e-04, eta: 0:00:33, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8155
2023-02-05 22:44:40,952 - mmcls - INFO - Epoch [3][22/72]	lr: 1.000e-04, eta: 0:00:32, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8966
2023-02-05 22:44:41,079 - mmcls - INFO - Epoch [3][23/72]	lr: 1.000e-04, eta: 0:00:32, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1688
2023-02-05 22:44:41,205 - mmcls - INFO - Epoch [3][24/72]	lr: 1.000e-04, eta: 0:00:32, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.2385
2023-02-05 22:44:41,331 - mmcls - INFO - Epoch [3][25/72]	lr: 1.000e-04, eta: 0:00:32, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.2247
2023-02-05 22:44:41,457 - mmcls - INFO - Epoch [3][26/72]	lr: 1.000e-04, eta: 0:00:31, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8925
2023-02-05 22:44:41,583 - mmcls - INFO - Epoch [3][27/72]	lr: 1.000e-04, eta: 0:00:31, time: 0.125, data_time: 0.008, memory: 3885, loss: 0.9486
2023-02-05 22:44:41,708 - mmcls - INFO - Epoch [3][28/72]	lr: 1.000e-04, eta: 0:00:31, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9172
2023-02-05 22:44:41,834 - mmcls - INFO - Epoch [3][29/72]	lr: 1.000e-04, eta: 0:00:31, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1562
2023-02-05 22:44:41,961 - mmcls - INFO - Epoch [3][30/72]	lr: 1.000e-04, eta: 0:00:31, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1743
2023-02-05 22:44:42,086 - mmcls - INFO - Epoch [3][31/72]	lr: 1.000e-04, eta: 0:00:30, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9699
2023-02-05 22:44:42,212 - mmcls - INFO - Epoch [3][32/72]	lr: 1.000e-04, eta: 0:00:30, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9194
2023-02-05 22:44:42,338 - mmcls - INFO - Epoch [3][33/72]	lr: 1.000e-04, eta: 0:00:30, time: 0.126, data_time: 0.009, memory: 3885, loss: 1.0685
2023-02-05 22:44:42,466 - mmcls - INFO - Epoch [3][34/72]	lr: 1.000e-04, eta: 0:00:30, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0778
2023-02-05 22:44:42,592 - mmcls - INFO - Epoch [3][35/72]	lr: 1.000e-04, eta: 0:00:30, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0449
2023-02-05 22:44:42,718 - mmcls - INFO - Epoch [3][36/72]	lr: 1.000e-04, eta: 0:00:29, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.1113
2023-02-05 22:44:42,844 - mmcls - INFO - Epoch [3][37/72]	lr: 1.000e-04, eta: 0:00:29, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2072
2023-02-05 22:44:42,970 - mmcls - INFO - Epoch [3][38/72]	lr: 1.000e-04, eta: 0:00:29, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1558
2023-02-05 22:44:43,098 - mmcls - INFO - Epoch [3][39/72]	lr: 1.000e-04, eta: 0:00:29, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2908
2023-02-05 22:44:43,223 - mmcls - INFO - Epoch [3][40/72]	lr: 1.000e-04, eta: 0:00:29, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.1702
2023-02-05 22:44:43,350 - mmcls - INFO - Epoch [3][41/72]	lr: 1.000e-04, eta: 0:00:28, time: 0.127, data_time: 0.009, memory: 3885, loss: 0.9223
2023-02-05 22:44:43,476 - mmcls - INFO - Epoch [3][42/72]	lr: 1.000e-04, eta: 0:00:28, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0230
2023-02-05 22:44:43,602 - mmcls - INFO - Epoch [3][43/72]	lr: 1.000e-04, eta: 0:00:28, time: 0.125, data_time: 0.008, memory: 3885, loss: 0.9249
2023-02-05 22:44:43,727 - mmcls - INFO - Epoch [3][44/72]	lr: 1.000e-04, eta: 0:00:28, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2625
2023-02-05 22:44:43,853 - mmcls - INFO - Epoch [3][45/72]	lr: 1.000e-04, eta: 0:00:28, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1669
2023-02-05 22:44:43,979 - mmcls - INFO - Epoch [3][46/72]	lr: 1.000e-04, eta: 0:00:27, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0512
2023-02-05 22:44:44,105 - mmcls - INFO - Epoch [3][47/72]	lr: 1.000e-04, eta: 0:00:27, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0487
2023-02-05 22:44:44,231 - mmcls - INFO - Epoch [3][48/72]	lr: 1.000e-04, eta: 0:00:27, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2280
2023-02-05 22:44:44,359 - mmcls - INFO - Epoch [3][49/72]	lr: 1.000e-04, eta: 0:00:27, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.2004
2023-02-05 22:44:44,485 - mmcls - INFO - Epoch [3][50/72]	lr: 1.000e-04, eta: 0:00:27, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9941
2023-02-05 22:44:44,611 - mmcls - INFO - Epoch [3][51/72]	lr: 1.000e-04, eta: 0:00:26, time: 0.126, data_time: 0.009, memory: 3885, loss: 1.0246
2023-02-05 22:44:44,737 - mmcls - INFO - Epoch [3][52/72]	lr: 1.000e-04, eta: 0:00:26, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.7440
2023-02-05 22:44:44,863 - mmcls - INFO - Epoch [3][53/72]	lr: 1.000e-04, eta: 0:00:26, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1258
2023-02-05 22:44:44,989 - mmcls - INFO - Epoch [3][54/72]	lr: 1.000e-04, eta: 0:00:26, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9763
2023-02-05 22:44:45,115 - mmcls - INFO - Epoch [3][55/72]	lr: 1.000e-04, eta: 0:00:26, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9169
2023-02-05 22:44:45,240 - mmcls - INFO - Epoch [3][56/72]	lr: 1.000e-04, eta: 0:00:25, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.0489
2023-02-05 22:44:45,368 - mmcls - INFO - Epoch [3][57/72]	lr: 1.000e-04, eta: 0:00:25, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.1173
2023-02-05 22:44:45,494 - mmcls - INFO - Epoch [3][58/72]	lr: 1.000e-04, eta: 0:00:25, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.7056
2023-02-05 22:44:45,619 - mmcls - INFO - Epoch [3][59/72]	lr: 1.000e-04, eta: 0:00:25, time: 0.125, data_time: 0.008, memory: 3885, loss: 0.8577
2023-02-05 22:44:45,746 - mmcls - INFO - Epoch [3][60/72]	lr: 1.000e-04, eta: 0:00:25, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1333
2023-02-05 22:44:45,872 - mmcls - INFO - Epoch [3][61/72]	lr: 1.000e-04, eta: 0:00:24, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9268
2023-02-05 22:44:45,997 - mmcls - INFO - Epoch [3][62/72]	lr: 1.000e-04, eta: 0:00:24, time: 0.125, data_time: 0.008, memory: 3885, loss: 0.9638
2023-02-05 22:44:46,123 - mmcls - INFO - Epoch [3][63/72]	lr: 1.000e-04, eta: 0:00:24, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1138
2023-02-05 22:44:46,249 - mmcls - INFO - Epoch [3][64/72]	lr: 1.000e-04, eta: 0:00:24, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2024
2023-02-05 22:44:46,377 - mmcls - INFO - Epoch [3][65/72]	lr: 1.000e-04, eta: 0:00:24, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.9973
2023-02-05 22:44:46,504 - mmcls - INFO - Epoch [3][66/72]	lr: 1.000e-04, eta: 0:00:24, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1067
2023-02-05 22:44:46,630 - mmcls - INFO - Epoch [3][67/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0837
2023-02-05 22:44:46,757 - mmcls - INFO - Epoch [3][68/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1036
2023-02-05 22:44:46,883 - mmcls - INFO - Epoch [3][69/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2560
2023-02-05 22:44:47,009 - mmcls - INFO - Epoch [3][70/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.126, data_time: 0.009, memory: 3885, loss: 1.1440
2023-02-05 22:44:47,134 - mmcls - INFO - Epoch [3][71/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.1200
2023-02-05 22:44:47,171 - mmcls - INFO - Epoch [3][72/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.044, data_time: 0.008, memory: 3885, loss: 0.9821
2023-02-05 22:44:47,172 - mmcls - INFO - Saving checkpoint at 3 epochs
2023-02-05 22:44:48,920 - mmcls - INFO - Epoch(val) [3][18]	accuracy_top-1: 93.1818, accuracy_top-5: 100.0000
2023-02-05 22:44:51,250 - mmcls - INFO - Epoch [4][1/72]	lr: 1.000e-04, eta: 0:00:24, time: 2.320, data_time: 2.193, memory: 3885, loss: 0.9679
2023-02-05 22:44:51,387 - mmcls - INFO - Epoch [4][2/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.139, data_time: 0.016, memory: 3885, loss: 1.1024
2023-02-05 22:44:51,515 - mmcls - INFO - Epoch [4][3/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.9768
2023-02-05 22:44:51,642 - mmcls - INFO - Epoch [4][4/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0343
2023-02-05 22:44:51,770 - mmcls - INFO - Epoch [4][5/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.7110
2023-02-05 22:44:51,896 - mmcls - INFO - Epoch [4][6/72]	lr: 1.000e-04, eta: 0:00:23, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0662
2023-02-05 22:44:52,025 - mmcls - INFO - Epoch [4][7/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.129, data_time: 0.008, memory: 3885, loss: 0.7663
2023-02-05 22:44:52,152 - mmcls - INFO - Epoch [4][8/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2411
2023-02-05 22:44:52,278 - mmcls - INFO - Epoch [4][9/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2228
2023-02-05 22:44:52,408 - mmcls - INFO - Epoch [4][10/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.130, data_time: 0.009, memory: 3885, loss: 0.8490
2023-02-05 22:44:52,535 - mmcls - INFO - Epoch [4][11/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0943
2023-02-05 22:44:52,663 - mmcls - INFO - Epoch [4][12/72]	lr: 1.000e-04, eta: 0:00:22, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1265
2023-02-05 22:44:52,790 - mmcls - INFO - Epoch [4][13/72]	lr: 1.000e-04, eta: 0:00:21, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1637
2023-02-05 22:44:52,918 - mmcls - INFO - Epoch [4][14/72]	lr: 1.000e-04, eta: 0:00:21, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0683
2023-02-05 22:44:53,046 - mmcls - INFO - Epoch [4][15/72]	lr: 1.000e-04, eta: 0:00:21, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2609
2023-02-05 22:44:53,173 - mmcls - INFO - Epoch [4][16/72]	lr: 1.000e-04, eta: 0:00:21, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9935
2023-02-05 22:44:53,301 - mmcls - INFO - Epoch [4][17/72]	lr: 1.000e-04, eta: 0:00:21, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.8928
2023-02-05 22:44:53,431 - mmcls - INFO - Epoch [4][18/72]	lr: 1.000e-04, eta: 0:00:20, time: 0.130, data_time: 0.009, memory: 3885, loss: 1.1288
2023-02-05 22:44:53,559 - mmcls - INFO - Epoch [4][19/72]	lr: 1.000e-04, eta: 0:00:20, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0767
2023-02-05 22:44:53,689 - mmcls - INFO - Epoch [4][20/72]	lr: 1.000e-04, eta: 0:00:20, time: 0.130, data_time: 0.008, memory: 3885, loss: 0.6878
2023-02-05 22:44:53,817 - mmcls - INFO - Epoch [4][21/72]	lr: 1.000e-04, eta: 0:00:20, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.8078
2023-02-05 22:44:53,944 - mmcls - INFO - Epoch [4][22/72]	lr: 1.000e-04, eta: 0:00:20, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0923
2023-02-05 22:44:54,071 - mmcls - INFO - Epoch [4][23/72]	lr: 1.000e-04, eta: 0:00:19, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0758
2023-02-05 22:44:54,199 - mmcls - INFO - Epoch [4][24/72]	lr: 1.000e-04, eta: 0:00:19, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.7948
2023-02-05 22:44:54,327 - mmcls - INFO - Epoch [4][25/72]	lr: 1.000e-04, eta: 0:00:19, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.7115
2023-02-05 22:44:54,455 - mmcls - INFO - Epoch [4][26/72]	lr: 1.000e-04, eta: 0:00:19, time: 0.129, data_time: 0.009, memory: 3885, loss: 0.7911
2023-02-05 22:44:54,583 - mmcls - INFO - Epoch [4][27/72]	lr: 1.000e-04, eta: 0:00:19, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1125
2023-02-05 22:44:54,710 - mmcls - INFO - Epoch [4][28/72]	lr: 1.000e-04, eta: 0:00:19, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1866
2023-02-05 22:44:54,837 - mmcls - INFO - Epoch [4][29/72]	lr: 1.000e-04, eta: 0:00:18, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2070
2023-02-05 22:44:54,963 - mmcls - INFO - Epoch [4][30/72]	lr: 1.000e-04, eta: 0:00:18, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1754
2023-02-05 22:44:55,092 - mmcls - INFO - Epoch [4][31/72]	lr: 1.000e-04, eta: 0:00:18, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0042
2023-02-05 22:44:55,220 - mmcls - INFO - Epoch [4][32/72]	lr: 1.000e-04, eta: 0:00:18, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.0134
2023-02-05 22:44:55,347 - mmcls - INFO - Epoch [4][33/72]	lr: 1.000e-04, eta: 0:00:18, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0004
2023-02-05 22:44:55,475 - mmcls - INFO - Epoch [4][34/72]	lr: 1.000e-04, eta: 0:00:17, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.1320
2023-02-05 22:44:55,603 - mmcls - INFO - Epoch [4][35/72]	lr: 1.000e-04, eta: 0:00:17, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1202
2023-02-05 22:44:55,731 - mmcls - INFO - Epoch [4][36/72]	lr: 1.000e-04, eta: 0:00:17, time: 0.129, data_time: 0.008, memory: 3885, loss: 1.2531
2023-02-05 22:44:55,857 - mmcls - INFO - Epoch [4][37/72]	lr: 1.000e-04, eta: 0:00:17, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2515
2023-02-05 22:44:55,984 - mmcls - INFO - Epoch [4][38/72]	lr: 1.000e-04, eta: 0:00:17, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8555
2023-02-05 22:44:56,112 - mmcls - INFO - Epoch [4][39/72]	lr: 1.000e-04, eta: 0:00:17, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0145
2023-02-05 22:44:56,238 - mmcls - INFO - Epoch [4][40/72]	lr: 1.000e-04, eta: 0:00:16, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1184
2023-02-05 22:44:56,365 - mmcls - INFO - Epoch [4][41/72]	lr: 1.000e-04, eta: 0:00:16, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8468
2023-02-05 22:44:56,492 - mmcls - INFO - Epoch [4][42/72]	lr: 1.000e-04, eta: 0:00:16, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.1111
2023-02-05 22:44:56,619 - mmcls - INFO - Epoch [4][43/72]	lr: 1.000e-04, eta: 0:00:16, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9383
2023-02-05 22:44:56,746 - mmcls - INFO - Epoch [4][44/72]	lr: 1.000e-04, eta: 0:00:16, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0378
2023-02-05 22:44:56,874 - mmcls - INFO - Epoch [4][45/72]	lr: 1.000e-04, eta: 0:00:16, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.2286
2023-02-05 22:44:57,001 - mmcls - INFO - Epoch [4][46/72]	lr: 1.000e-04, eta: 0:00:15, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.5948
2023-02-05 22:44:57,129 - mmcls - INFO - Epoch [4][47/72]	lr: 1.000e-04, eta: 0:00:15, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.8310
2023-02-05 22:44:57,256 - mmcls - INFO - Epoch [4][48/72]	lr: 1.000e-04, eta: 0:00:15, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3381
2023-02-05 22:44:57,384 - mmcls - INFO - Epoch [4][49/72]	lr: 1.000e-04, eta: 0:00:15, time: 0.128, data_time: 0.010, memory: 3885, loss: 0.6161
2023-02-05 22:44:57,512 - mmcls - INFO - Epoch [4][50/72]	lr: 1.000e-04, eta: 0:00:15, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.9070
2023-02-05 22:44:57,639 - mmcls - INFO - Epoch [4][51/72]	lr: 1.000e-04, eta: 0:00:14, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2237
2023-02-05 22:44:57,766 - mmcls - INFO - Epoch [4][52/72]	lr: 1.000e-04, eta: 0:00:14, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9938
2023-02-05 22:44:57,893 - mmcls - INFO - Epoch [4][53/72]	lr: 1.000e-04, eta: 0:00:14, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8029
2023-02-05 22:44:58,021 - mmcls - INFO - Epoch [4][54/72]	lr: 1.000e-04, eta: 0:00:14, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.7626
2023-02-05 22:44:58,150 - mmcls - INFO - Epoch [4][55/72]	lr: 1.000e-04, eta: 0:00:14, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.0534
2023-02-05 22:44:58,278 - mmcls - INFO - Epoch [4][56/72]	lr: 1.000e-04, eta: 0:00:14, time: 0.129, data_time: 0.008, memory: 3885, loss: 0.7420
2023-02-05 22:44:58,406 - mmcls - INFO - Epoch [4][57/72]	lr: 1.000e-04, eta: 0:00:13, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.1078
2023-02-05 22:44:58,534 - mmcls - INFO - Epoch [4][58/72]	lr: 1.000e-04, eta: 0:00:13, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1178
2023-02-05 22:44:58,662 - mmcls - INFO - Epoch [4][59/72]	lr: 1.000e-04, eta: 0:00:13, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2625
2023-02-05 22:44:58,791 - mmcls - INFO - Epoch [4][60/72]	lr: 1.000e-04, eta: 0:00:13, time: 0.129, data_time: 0.008, memory: 3885, loss: 0.6783
2023-02-05 22:44:58,918 - mmcls - INFO - Epoch [4][61/72]	lr: 1.000e-04, eta: 0:00:13, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.9128
2023-02-05 22:44:59,046 - mmcls - INFO - Epoch [4][62/72]	lr: 1.000e-04, eta: 0:00:13, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1786
2023-02-05 22:44:59,172 - mmcls - INFO - Epoch [4][63/72]	lr: 1.000e-04, eta: 0:00:12, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0737
2023-02-05 22:44:59,299 - mmcls - INFO - Epoch [4][64/72]	lr: 1.000e-04, eta: 0:00:12, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1998
2023-02-05 22:44:59,427 - mmcls - INFO - Epoch [4][65/72]	lr: 1.000e-04, eta: 0:00:12, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.1916
2023-02-05 22:44:59,555 - mmcls - INFO - Epoch [4][66/72]	lr: 1.000e-04, eta: 0:00:12, time: 0.128, data_time: 0.008, memory: 3885, loss: 1.1387
2023-02-05 22:44:59,682 - mmcls - INFO - Epoch [4][67/72]	lr: 1.000e-04, eta: 0:00:12, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8341
2023-02-05 22:44:59,810 - mmcls - INFO - Epoch [4][68/72]	lr: 1.000e-04, eta: 0:00:12, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1646
2023-02-05 22:44:59,936 - mmcls - INFO - Epoch [4][69/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8068
2023-02-05 22:45:00,061 - mmcls - INFO - Epoch [4][70/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0313
2023-02-05 22:45:00,187 - mmcls - INFO - Epoch [4][71/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9164
2023-02-05 22:45:00,225 - mmcls - INFO - Epoch [4][72/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.045, data_time: 0.009, memory: 3885, loss: 0.6601
2023-02-05 22:45:00,225 - mmcls - INFO - Saving checkpoint at 4 epochs
2023-02-05 22:45:01,985 - mmcls - INFO - Epoch(val) [4][18]	accuracy_top-1: 93.1818, accuracy_top-5: 100.0000
2023-02-05 22:45:04,317 - mmcls - INFO - Epoch [5][1/72]	lr: 1.000e-04, eta: 0:00:11, time: 2.324, data_time: 2.197, memory: 3885, loss: 1.0908
2023-02-05 22:45:04,448 - mmcls - INFO - Epoch [5][2/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.131, data_time: 0.009, memory: 3885, loss: 1.2194
2023-02-05 22:45:04,574 - mmcls - INFO - Epoch [5][3/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.125, data_time: 0.008, memory: 3885, loss: 1.0561
2023-02-05 22:45:04,700 - mmcls - INFO - Epoch [5][4/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.126, data_time: 0.009, memory: 3885, loss: 0.9878
2023-02-05 22:45:04,826 - mmcls - INFO - Epoch [5][5/72]	lr: 1.000e-04, eta: 0:00:11, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3869
2023-02-05 22:45:04,952 - mmcls - INFO - Epoch [5][6/72]	lr: 1.000e-04, eta: 0:00:10, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8427
2023-02-05 22:45:05,079 - mmcls - INFO - Epoch [5][7/72]	lr: 1.000e-04, eta: 0:00:10, time: 0.127, data_time: 0.009, memory: 3885, loss: 1.0321
2023-02-05 22:45:05,205 - mmcls - INFO - Epoch [5][8/72]	lr: 1.000e-04, eta: 0:00:10, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1464
2023-02-05 22:45:05,331 - mmcls - INFO - Epoch [5][9/72]	lr: 1.000e-04, eta: 0:00:10, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9945
2023-02-05 22:45:05,458 - mmcls - INFO - Epoch [5][10/72]	lr: 1.000e-04, eta: 0:00:10, time: 0.128, data_time: 0.010, memory: 3885, loss: 1.0463
2023-02-05 22:45:05,584 - mmcls - INFO - Epoch [5][11/72]	lr: 1.000e-04, eta: 0:00:10, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0233
2023-02-05 22:45:05,710 - mmcls - INFO - Epoch [5][12/72]	lr: 1.000e-04, eta: 0:00:09, time: 0.125, data_time: 0.008, memory: 3885, loss: 0.9608
2023-02-05 22:45:05,836 - mmcls - INFO - Epoch [5][13/72]	lr: 1.000e-04, eta: 0:00:09, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0840
2023-02-05 22:45:05,963 - mmcls - INFO - Epoch [5][14/72]	lr: 1.000e-04, eta: 0:00:09, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.9248
2023-02-05 22:45:06,089 - mmcls - INFO - Epoch [5][15/72]	lr: 1.000e-04, eta: 0:00:09, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8875
2023-02-05 22:45:06,215 - mmcls - INFO - Epoch [5][16/72]	lr: 1.000e-04, eta: 0:00:09, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9635
2023-02-05 22:45:06,341 - mmcls - INFO - Epoch [5][17/72]	lr: 1.000e-04, eta: 0:00:09, time: 0.126, data_time: 0.009, memory: 3885, loss: 1.1846
2023-02-05 22:45:06,469 - mmcls - INFO - Epoch [5][18/72]	lr: 1.000e-04, eta: 0:00:08, time: 0.128, data_time: 0.010, memory: 3885, loss: 1.1909
2023-02-05 22:45:06,595 - mmcls - INFO - Epoch [5][19/72]	lr: 1.000e-04, eta: 0:00:08, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8987
2023-02-05 22:45:06,722 - mmcls - INFO - Epoch [5][20/72]	lr: 1.000e-04, eta: 0:00:08, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.6223
2023-02-05 22:45:06,849 - mmcls - INFO - Epoch [5][21/72]	lr: 1.000e-04, eta: 0:00:08, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0081
2023-02-05 22:45:06,975 - mmcls - INFO - Epoch [5][22/72]	lr: 1.000e-04, eta: 0:00:08, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0706
2023-02-05 22:45:07,101 - mmcls - INFO - Epoch [5][23/72]	lr: 1.000e-04, eta: 0:00:07, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0976
2023-02-05 22:45:07,228 - mmcls - INFO - Epoch [5][24/72]	lr: 1.000e-04, eta: 0:00:07, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2799
2023-02-05 22:45:07,355 - mmcls - INFO - Epoch [5][25/72]	lr: 1.000e-04, eta: 0:00:07, time: 0.127, data_time: 0.009, memory: 3885, loss: 0.8741
2023-02-05 22:45:07,483 - mmcls - INFO - Epoch [5][26/72]	lr: 1.000e-04, eta: 0:00:07, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.7746
2023-02-05 22:45:07,609 - mmcls - INFO - Epoch [5][27/72]	lr: 1.000e-04, eta: 0:00:07, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1904
2023-02-05 22:45:07,736 - mmcls - INFO - Epoch [5][28/72]	lr: 1.000e-04, eta: 0:00:07, time: 0.126, data_time: 0.009, memory: 3885, loss: 0.8069
2023-02-05 22:45:07,864 - mmcls - INFO - Epoch [5][29/72]	lr: 1.000e-04, eta: 0:00:06, time: 0.128, data_time: 0.008, memory: 3885, loss: 0.9398
2023-02-05 22:45:07,989 - mmcls - INFO - Epoch [5][30/72]	lr: 1.000e-04, eta: 0:00:06, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9821
2023-02-05 22:45:08,116 - mmcls - INFO - Epoch [5][31/72]	lr: 1.000e-04, eta: 0:00:06, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1987
2023-02-05 22:45:08,244 - mmcls - INFO - Epoch [5][32/72]	lr: 1.000e-04, eta: 0:00:06, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.9629
2023-02-05 22:45:08,370 - mmcls - INFO - Epoch [5][33/72]	lr: 1.000e-04, eta: 0:00:06, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9604
2023-02-05 22:45:08,498 - mmcls - INFO - Epoch [5][34/72]	lr: 1.000e-04, eta: 0:00:06, time: 0.129, data_time: 0.010, memory: 3885, loss: 1.1693
2023-02-05 22:45:08,625 - mmcls - INFO - Epoch [5][35/72]	lr: 1.000e-04, eta: 0:00:05, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0133
2023-02-05 22:45:08,752 - mmcls - INFO - Epoch [5][36/72]	lr: 1.000e-04, eta: 0:00:05, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1454
2023-02-05 22:45:08,879 - mmcls - INFO - Epoch [5][37/72]	lr: 1.000e-04, eta: 0:00:05, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8754
2023-02-05 22:45:09,006 - mmcls - INFO - Epoch [5][38/72]	lr: 1.000e-04, eta: 0:00:05, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.3011
2023-02-05 22:45:09,133 - mmcls - INFO - Epoch [5][39/72]	lr: 1.000e-04, eta: 0:00:05, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0719
2023-02-05 22:45:09,260 - mmcls - INFO - Epoch [5][40/72]	lr: 1.000e-04, eta: 0:00:05, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1564
2023-02-05 22:45:09,386 - mmcls - INFO - Epoch [5][41/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1620
2023-02-05 22:45:09,513 - mmcls - INFO - Epoch [5][42/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.9352
2023-02-05 22:45:09,639 - mmcls - INFO - Epoch [5][43/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9715
2023-02-05 22:45:09,765 - mmcls - INFO - Epoch [5][44/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.8467
2023-02-05 22:45:09,893 - mmcls - INFO - Epoch [5][45/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.127, data_time: 0.009, memory: 3885, loss: 0.8956
2023-02-05 22:45:10,019 - mmcls - INFO - Epoch [5][46/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0603
2023-02-05 22:45:10,146 - mmcls - INFO - Epoch [5][47/72]	lr: 1.000e-04, eta: 0:00:04, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0329
2023-02-05 22:45:10,272 - mmcls - INFO - Epoch [5][48/72]	lr: 1.000e-04, eta: 0:00:03, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9993
2023-02-05 22:45:10,398 - mmcls - INFO - Epoch [5][49/72]	lr: 1.000e-04, eta: 0:00:03, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1145
2023-02-05 22:45:10,527 - mmcls - INFO - Epoch [5][50/72]	lr: 1.000e-04, eta: 0:00:03, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.1669
2023-02-05 22:45:10,653 - mmcls - INFO - Epoch [5][51/72]	lr: 1.000e-04, eta: 0:00:03, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1402
2023-02-05 22:45:10,779 - mmcls - INFO - Epoch [5][52/72]	lr: 1.000e-04, eta: 0:00:03, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9328
2023-02-05 22:45:10,905 - mmcls - INFO - Epoch [5][53/72]	lr: 1.000e-04, eta: 0:00:03, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1536
2023-02-05 22:45:11,032 - mmcls - INFO - Epoch [5][54/72]	lr: 1.000e-04, eta: 0:00:02, time: 0.127, data_time: 0.008, memory: 3885, loss: 0.8555
2023-02-05 22:45:11,159 - mmcls - INFO - Epoch [5][55/72]	lr: 1.000e-04, eta: 0:00:02, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.1123
2023-02-05 22:45:11,287 - mmcls - INFO - Epoch [5][56/72]	lr: 1.000e-04, eta: 0:00:02, time: 0.128, data_time: 0.009, memory: 3885, loss: 1.1083
2023-02-05 22:45:11,413 - mmcls - INFO - Epoch [5][57/72]	lr: 1.000e-04, eta: 0:00:02, time: 0.126, data_time: 0.009, memory: 3885, loss: 0.7488
2023-02-05 22:45:11,541 - mmcls - INFO - Epoch [5][58/72]	lr: 1.000e-04, eta: 0:00:02, time: 0.128, data_time: 0.009, memory: 3885, loss: 0.7755
2023-02-05 22:45:11,667 - mmcls - INFO - Epoch [5][59/72]	lr: 1.000e-04, eta: 0:00:02, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1308
2023-02-05 22:45:11,793 - mmcls - INFO - Epoch [5][60/72]	lr: 1.000e-04, eta: 0:00:01, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0579
2023-02-05 22:45:11,921 - mmcls - INFO - Epoch [5][61/72]	lr: 1.000e-04, eta: 0:00:01, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2173
2023-02-05 22:45:12,047 - mmcls - INFO - Epoch [5][62/72]	lr: 1.000e-04, eta: 0:00:01, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.0230
2023-02-05 22:45:12,174 - mmcls - INFO - Epoch [5][63/72]	lr: 1.000e-04, eta: 0:00:01, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.2983
2023-02-05 22:45:12,300 - mmcls - INFO - Epoch [5][64/72]	lr: 1.000e-04, eta: 0:00:01, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2477
2023-02-05 22:45:12,427 - mmcls - INFO - Epoch [5][65/72]	lr: 1.000e-04, eta: 0:00:01, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0690
2023-02-05 22:45:12,555 - mmcls - INFO - Epoch [5][66/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.128, data_time: 0.010, memory: 3885, loss: 0.7410
2023-02-05 22:45:12,681 - mmcls - INFO - Epoch [5][67/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.1168
2023-02-05 22:45:12,808 - mmcls - INFO - Epoch [5][68/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.127, data_time: 0.008, memory: 3885, loss: 1.2039
2023-02-05 22:45:12,934 - mmcls - INFO - Epoch [5][69/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.126, data_time: 0.008, memory: 3885, loss: 1.0234
2023-02-05 22:45:13,060 - mmcls - INFO - Epoch [5][70/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.6517
2023-02-05 22:45:13,186 - mmcls - INFO - Epoch [5][71/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.126, data_time: 0.008, memory: 3885, loss: 0.9761
2023-02-05 22:45:13,223 - mmcls - INFO - Epoch [5][72/72]	lr: 1.000e-04, eta: 0:00:00, time: 0.044, data_time: 0.008, memory: 3885, loss: 0.9673
2023-02-05 22:45:13,224 - mmcls - INFO - Saving checkpoint at 5 epochs
2023-02-05 22:45:14,986 - mmcls - INFO - Epoch(val) [5][18]	accuracy_top-1: 94.2308, accuracy_top-5: 100.0000
